{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- YouTube Video Stats ---\n",
      "Title: Super Bowl LX Opening Performance by Green Day\n",
      "Channel: NFL\n",
      "Published: 2026-02-09T01:59:30Z\n",
      "Views: 4147596\n",
      "Likes: 124076\n",
      "Comments: 6604\n",
      "Duration: PT5M12S\n",
      "Description: Watch live local and primetime games, NFL RedZone, and NFL Network on Plus.NFL.com\n",
      "\n",
      "Check out our other channels:\n",
      "NFL Mundo https://www.youtube.com/mundonfl\n",
      "NFL Brasil https://www.youtube.com/c/NFLBra...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "API_KEY = \"AIzaSyDD9KTnN9b-ngp2OCPysSEbHPn10MpnxfA\"\n",
    "VIDEO_URL = \"https://www.youtube.com/watch?v=_0UqnYmQyLQ\"\n",
    "\n",
    "# Extract Video ID from URL\n",
    "VIDEO_ID = VIDEO_URL.split(\"v=\")[1].split(\"&\")[0]  # Added .split(\"&\")[0] to handle URLs with additional parameters\n",
    "\n",
    "# Call YouTube API\n",
    "url = \"https://www.googleapis.com/youtube/v3/videos\"\n",
    "params = {\n",
    "    \"part\": \"snippet,statistics,contentDetails\",\n",
    "    \"id\": VIDEO_ID,\n",
    "    \"key\": API_KEY\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params).json()\n",
    "\n",
    "if \"items\" not in response or len(response[\"items\"]) == 0:\n",
    "    print(\"Video not found or API error\")\n",
    "    if \"error\" in response:\n",
    "        print(\"Error details:\", response[\"error\"])\n",
    "else:\n",
    "    video = response[\"items\"][0]\n",
    "    title = video[\"snippet\"][\"title\"]\n",
    "    channel = video[\"snippet\"][\"channelTitle\"]\n",
    "    published = video[\"snippet\"][\"publishedAt\"]\n",
    "    description = video[\"snippet\"][\"description\"]\n",
    "    views = video[\"statistics\"].get(\"viewCount\")\n",
    "    likes = video[\"statistics\"].get(\"likeCount\")\n",
    "    comments = video[\"statistics\"].get(\"commentCount\")\n",
    "    duration = video[\"contentDetails\"][\"duration\"]\n",
    "    \n",
    "    print(\"\\n--- YouTube Video Stats ---\")\n",
    "    print(\"Title:\", title)\n",
    "    print(\"Channel:\", channel)\n",
    "    print(\"Published:\", published)\n",
    "    print(\"Views:\", views)\n",
    "    print(\"Likes:\", likes)\n",
    "    print(\"Comments:\", comments)\n",
    "    print(\"Duration:\", duration)\n",
    "    print(\"Description:\", description[:200] + \"...\" if len(description) > 200 else description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dependencies...\n",
      "\n",
      "‚úó textblob not found\n",
      "Installing textblob...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script tqdm is installed in '/Users/adityakiran/Library/Python/3.9/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script nltk is installed in '/Users/adityakiran/Library/Python/3.9/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì textblob installed successfully\n",
      "Downloading TextBlob corpora...\n",
      "‚úì TextBlob corpora downloaded\n",
      "‚úó matplotlib not found\n",
      "Installing matplotlib...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The scripts fonttools, pyftmerge, pyftsubset and ttx are installed in '/Users/adityakiran/Library/Python/3.9/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì matplotlib installed successfully\n",
      "‚úó seaborn not found\n",
      "Installing seaborn...\n",
      "‚úì seaborn installed successfully\n",
      "‚úó wordcloud not found\n",
      "Installing wordcloud...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script wordcloud_cli is installed in '/Users/adityakiran/Library/Python/3.9/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì wordcloud installed successfully\n",
      "‚úó openpyxl not found\n",
      "Installing openpyxl...\n",
      "‚úì openpyxl installed successfully\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YouTube Comments Sentiment Analysis\n",
      "================================================================================\n",
      "\n",
      "Loading comments data...\n",
      "‚úì Loading: youtube_comments__0UqnYmQyLQ_20260211_032713.csv\n",
      "\n",
      "‚úì Loaded 1,000 comments\n",
      "\n",
      "Analyzing sentiments...\n",
      "  Progress: 1000/1000 comments analyzed (100.0%)\n",
      "Detecting emotions...\n",
      "  Progress: 800/1000 comments analyzed (80.0%)\r  Progress: 900/1000 comments analyzed (90.0%)\r  Progress: 1000/1000 comments analyzed (100.0%)\r\n",
      "Calculating toxicity scores...\n",
      "  Progress: 100/1000 comments analyzed (10.0%)\r  Progress: 200/1000 comments analyzed (20.0%)\r  Progress: 300/1000 comments analyzed (30.0%)\r  Progress: 400/1000 comments analyzed (40.0%)\r  Progress: 500/1000 comments analyzed (50.0%)\r  Progress: 600/1000 comments analyzed (60.0%)\r  Progress: 700/1000 comments analyzed (70.0%)\r  Progress: 800/1000 comments analyzed (80.0%)\r  Progress: 900/1000 comments analyzed (90.0%)\r  Progress: 1000/1000 comments analyzed (100.0%)\r\n",
      "\n",
      "Generating insights...\n",
      "\n",
      "Creating visualizations...\n",
      "‚úì Saved visualizations to sentiment_analysis_visualizations.png\n",
      "‚úì Saved word clouds to sentiment_analysis_wordclouds.png\n",
      "\n",
      "================================================================================\n",
      "SENTIMENT ANALYSIS REPORT\n",
      "================================================================================\n",
      "\n",
      "üìä OVERALL STATISTICS\n",
      "--------------------------------------------------------------------------------\n",
      "Total Comments Analyzed: 1,000\n",
      "Average Sentiment Score: 0.108 (Positive)\n",
      "Average Toxicity Score: 0.027\n",
      "\n",
      "üí≠ SENTIMENT DISTRIBUTION\n",
      "--------------------------------------------------------------------------------\n",
      "Neutral     :  43.2%\n",
      "Positive    :  39.2%\n",
      "Negative    :  17.6%\n",
      "\n",
      "üòä EMOTION DISTRIBUTION\n",
      "--------------------------------------------------------------------------------\n",
      "Neutral     :  60.1%\n",
      "Joy         :  30.8%\n",
      "Sadness     :   4.9%\n",
      "Anger       :   2.5%\n",
      "Surprise    :   1.5%\n",
      "Fear        :   0.2%\n",
      "\n",
      "üëç ENGAGEMENT BY SENTIMENT\n",
      "--------------------------------------------------------------------------------\n",
      "Positive comments: 163.2 avg likes\n",
      "Neutral comments:  224.0 avg likes\n",
      "Negative comments: 150.4 avg likes\n",
      "\n",
      "‚ö†Ô∏è  TOXICITY ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "High toxicity comments: 1 (0.1%)\n",
      "\n",
      "üåü TOP 3 MOST POSITIVE COMMENTS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üë§ @nataliecarry7300 | Score: 1.000 | Likes: 8\n",
      "   Best comment üéâüòÇ\n",
      "\n",
      "üë§ @ezekielspates5677 | Score: 1.000 | Likes: 5\n",
      "   ‚Äã@kennywonglabow3547Yes it was! That album was like a greatest hits album every song on there was awesome.\n",
      "\n",
      "üë§ @NixStox | Score: 1.000 | Likes: 2\n",
      "   This was perfect ... and not lip-sync\n",
      "\n",
      "üò† TOP 3 MOST NEGATIVE COMMENTS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "üë§ @fnfreyof2686 | Score: -1.000 | Likes: 19\n",
      "   Ya suban el de bad bunny!!\n",
      "\n",
      "üë§ @jenaemils | Score: -1.000 | Likes: 5\n",
      "   QUEREMOS VER A BAD BUNNY PR EN LA CASA PU√ëETA!!!! üáµüá∑üôå\n",
      "\n",
      "üë§ @Brenaguilaar | Score: -1.000 | Likes: 14\n",
      "   SUBAN EL DE BAD BUNNYYY!!!\n",
      "\n",
      "================================================================================\n",
      "\n",
      "‚úì Saved analyzed data to sentiment_analysis_results_20260211_033951.csv\n",
      "‚úì Saved insights to sentiment_analysis_insights_20260211_033951.json\n",
      "‚úì Saved Excel file to sentiment_analysis_results_20260211_033951.xlsx\n",
      "\n",
      "‚úÖ Sentiment analysis complete!\n",
      "\n",
      "Files generated:\n",
      "  üìÑ CSV with sentiment scores\n",
      "  üìä Excel with detailed breakdown\n",
      "  üìà Visualization charts\n",
      "  üí≠ Word clouds (if available)\n",
      "  üìã Insights JSON\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "YouTube Comments Sentiment Analysis\n",
    "Analyzes sentiment, emotion, and toxicity of extracted comments\n",
    "Automatically installs missing dependencies\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "def install_package(package_name):\n",
    "    \"\"\"Install a package using pip.\"\"\"\n",
    "    try:\n",
    "        print(f\"Installing {package_name}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name, \"--quiet\"])\n",
    "        print(f\"‚úì {package_name} installed successfully\")\n",
    "        return True\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"‚úó Failed to install {package_name}\")\n",
    "        return False\n",
    "\n",
    "def check_and_install_dependencies():\n",
    "    \"\"\"Check and install required packages.\"\"\"\n",
    "    required_packages = {\n",
    "        'textblob': 'textblob',\n",
    "        'matplotlib': 'matplotlib',\n",
    "        'seaborn': 'seaborn',\n",
    "        'wordcloud': 'wordcloud',\n",
    "        'openpyxl': 'openpyxl'\n",
    "    }\n",
    "    \n",
    "    print(\"Checking dependencies...\\n\")\n",
    "    \n",
    "    for module_name, package_name in required_packages.items():\n",
    "        try:\n",
    "            __import__(module_name)\n",
    "            print(f\"‚úì {module_name} is already installed\")\n",
    "        except ImportError:\n",
    "            print(f\"‚úó {module_name} not found\")\n",
    "            if install_package(package_name):\n",
    "                # For textblob, we need to download corpora\n",
    "                if module_name == 'textblob':\n",
    "                    print(\"Downloading TextBlob corpora...\")\n",
    "                    try:\n",
    "                        subprocess.check_call([sys.executable, \"-m\", \"textblob.download_corpora\", \"lite\"], \n",
    "                                            stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "                        print(\"‚úì TextBlob corpora downloaded\")\n",
    "                    except:\n",
    "                        print(\"‚ö† TextBlob corpora download failed - will use basic sentiment analysis\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Run dependency check\n",
    "check_and_install_dependencies()\n",
    "\n",
    "# Now import the packages\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "INPUT_FILE = \"youtube_comments_*.csv\"  # Your extracted comments file\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string\n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def analyze_sentiment_textblob(text):\n",
    "    \"\"\"\n",
    "    Analyze sentiment using TextBlob.\n",
    "    Returns polarity (-1 to 1) and subjectivity (0 to 1)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        blob = TextBlob(clean_text(text))\n",
    "        return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "    except:\n",
    "        return 0, 0\n",
    "\n",
    "def classify_sentiment(polarity):\n",
    "    \"\"\"Classify sentiment based on polarity score.\"\"\"\n",
    "    if polarity > 0.1:\n",
    "        return 'Positive'\n",
    "    elif polarity < -0.1:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "def detect_emotion(text):\n",
    "    \"\"\"\n",
    "    Simple rule-based emotion detection.\n",
    "    Returns primary emotion detected.\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Emotion keywords\n",
    "    emotion_keywords = {\n",
    "        'joy': ['happy', 'joy', 'love', 'loved', 'excited', 'amazing', 'awesome', 'great', 'wonderful', \n",
    "                'excellent', 'fantastic', 'best', 'perfect', 'beautiful', '‚ù§Ô∏è', '‚ù§', 'üòä', 'üòÑ', 'üéâ', \n",
    "                'üî•', 'üëè', 'üôå'],\n",
    "        'anger': ['angry', 'hate', 'hated', 'furious', 'mad', 'annoyed', 'frustrated', 'terrible', \n",
    "                  'worst', 'awful', 'horrible', 'trash', 'garbage', 'üò†', 'üò°', 'ü§¨', 'üí¢'],\n",
    "        'sadness': ['sad', 'disappointed', 'disappointing', 'sorry', 'unfortunate', 'depressed', \n",
    "                    'unhappy', 'miss', 'missed', 'crying', 'üò¢', 'üò≠', 'üòî', 'üíî'],\n",
    "        'fear': ['scared', 'afraid', 'worried', 'anxious', 'nervous', 'concerned', 'fear', 'terrified'],\n",
    "        'surprise': ['wow', 'omg', 'amazing', 'incredible', 'unbelievable', 'shocked', 'surprising', \n",
    "                     'wtf', 'üòÆ', 'üò≤', 'ü§Ø'],\n",
    "        'disgust': ['disgusting', 'gross', 'awful', 'horrible', 'nasty', 'sick', 'yuck', 'ü§Æ', 'ü§¢']\n",
    "    }\n",
    "    \n",
    "    emotion_scores = {}\n",
    "    for emotion, keywords in emotion_keywords.items():\n",
    "        score = sum(1 for keyword in keywords if keyword in text_lower)\n",
    "        emotion_scores[emotion] = score\n",
    "    \n",
    "    # Return emotion with highest score, or neutral if no emotions detected\n",
    "    max_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "    return max_emotion if emotion_scores[max_emotion] > 0 else 'neutral'\n",
    "\n",
    "def calculate_toxicity_score(text):\n",
    "    \"\"\"\n",
    "    Simple toxicity detection based on offensive keywords.\n",
    "    Returns score from 0 (not toxic) to 1 (highly toxic)\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Common toxic indicators\n",
    "    toxic_keywords = [\n",
    "        'hate', 'stupid', 'idiot', 'dumb', 'moron', 'terrible',\n",
    "        'worst', 'garbage', 'trash', 'suck', 'sucks', 'awful', 'horrible',\n",
    "        'pathetic', 'loser', 'failure', 'disgrace', 'embarrassing'\n",
    "    ]\n",
    "    \n",
    "    toxic_count = sum(1 for keyword in toxic_keywords if keyword in text_lower)\n",
    "    \n",
    "    # Normalize to 0-1 scale\n",
    "    toxicity = min(toxic_count / 3, 1.0)  # Cap at 1.0\n",
    "    \n",
    "    return toxicity\n",
    "\n",
    "def analyze_comments(df):\n",
    "    \"\"\"\n",
    "    Perform comprehensive sentiment analysis on comments.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with comments\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with sentiment analysis results\n",
    "    \"\"\"\n",
    "    print(\"Analyzing sentiments...\")\n",
    "    total = len(df)\n",
    "    \n",
    "    # Apply sentiment analysis with progress tracking\n",
    "    sentiments = []\n",
    "    for idx, text in enumerate(df['comment_text'], 1):\n",
    "        if idx % 100 == 0 or idx == total:\n",
    "            print(f\"  Progress: {idx}/{total} comments analyzed ({idx/total*100:.1f}%)\", end='\\r')\n",
    "        sentiments.append(analyze_sentiment_textblob(text))\n",
    "    \n",
    "    print()  # New line after progress\n",
    "    \n",
    "    df['sentiment_polarity'] = [s[0] for s in sentiments]\n",
    "    df['sentiment_subjectivity'] = [s[1] for s in sentiments]\n",
    "    \n",
    "    # Classify sentiment\n",
    "    df['sentiment'] = df['sentiment_polarity'].apply(classify_sentiment)\n",
    "    df['sentiment_score'] = df['sentiment_polarity']  # Alias for consistency\n",
    "    \n",
    "    # Detect emotions\n",
    "    print(\"Detecting emotions...\")\n",
    "    emotions = []\n",
    "    for idx, text in enumerate(df['comment_text'], 1):\n",
    "        if idx % 100 == 0 or idx == total:\n",
    "            print(f\"  Progress: {idx}/{total} comments analyzed ({idx/total*100:.1f}%)\", end='\\r')\n",
    "        emotions.append(detect_emotion(text))\n",
    "    print()  # New line\n",
    "    df['emotion'] = emotions\n",
    "    \n",
    "    # Calculate toxicity\n",
    "    print(\"Calculating toxicity scores...\")\n",
    "    toxicity = []\n",
    "    for idx, text in enumerate(df['comment_text'], 1):\n",
    "        if idx % 100 == 0 or idx == total:\n",
    "            print(f\"  Progress: {idx}/{total} comments analyzed ({idx/total*100:.1f}%)\", end='\\r')\n",
    "        toxicity.append(calculate_toxicity_score(text))\n",
    "    print()  # New line\n",
    "    df['toxicity_score'] = toxicity\n",
    "    \n",
    "    # Spam detection (simple heuristic)\n",
    "    df['spam_score'] = df.apply(lambda row: \n",
    "        1.0 if (row.get('has_url', False) and row['comment_length'] < 50) or \n",
    "               (row['comment_text'].count('!') > 5) or\n",
    "               (row.get('is_all_caps', False) and row['comment_length'] > 20)\n",
    "        else 0.0, axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_insights(df):\n",
    "    \"\"\"Generate insights from sentiment analysis.\"\"\"\n",
    "    insights = {}\n",
    "    \n",
    "    # Overall sentiment distribution\n",
    "    sentiment_dist = df['sentiment'].value_counts(normalize=True) * 100\n",
    "    insights['sentiment_distribution'] = sentiment_dist.to_dict()\n",
    "    \n",
    "    # Average sentiment score\n",
    "    insights['avg_sentiment_score'] = df['sentiment_polarity'].mean()\n",
    "    \n",
    "    # Emotion distribution\n",
    "    emotion_dist = df['emotion'].value_counts(normalize=True) * 100\n",
    "    insights['emotion_distribution'] = emotion_dist.to_dict()\n",
    "    \n",
    "    # Toxicity analysis\n",
    "    insights['avg_toxicity'] = df['toxicity_score'].mean()\n",
    "    insights['high_toxicity_count'] = len(df[df['toxicity_score'] > 0.5])\n",
    "    insights['high_toxicity_pct'] = (insights['high_toxicity_count'] / len(df)) * 100\n",
    "    \n",
    "    # Engagement vs Sentiment\n",
    "    insights['avg_likes_positive'] = df[df['sentiment'] == 'Positive']['like_count'].mean()\n",
    "    insights['avg_likes_negative'] = df[df['sentiment'] == 'Negative']['like_count'].mean()\n",
    "    insights['avg_likes_neutral'] = df[df['sentiment'] == 'Neutral']['like_count'].mean()\n",
    "    \n",
    "    # Time-based patterns if columns exist\n",
    "    if 'day_of_week' in df.columns:\n",
    "        sentiment_by_day = df.groupby('day_of_week')['sentiment_polarity'].mean().to_dict()\n",
    "        insights['sentiment_by_day'] = sentiment_by_day\n",
    "    \n",
    "    if 'hour_of_day' in df.columns:\n",
    "        sentiment_by_hour = df.groupby('hour_of_day')['sentiment_polarity'].mean().to_dict()\n",
    "        insights['sentiment_by_hour'] = sentiment_by_hour\n",
    "    \n",
    "    return insights\n",
    "\n",
    "def create_visualizations(df, output_prefix='sentiment_analysis'):\n",
    "    \"\"\"Create visualization plots.\"\"\"\n",
    "    print(\"\\nCreating visualizations...\")\n",
    "    \n",
    "    # Set style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    plt.rcParams['figure.figsize'] = (15, 10)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    \n",
    "    # 1. Sentiment Distribution\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    sentiment_counts = df['sentiment'].value_counts()\n",
    "    colors = {'Positive': '#2ecc71', 'Neutral': '#95a5a6', 'Negative': '#e74c3c'}\n",
    "    sentiment_colors = [colors[s] for s in sentiment_counts.index]\n",
    "    ax1.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%',\n",
    "            colors=sentiment_colors, startangle=90)\n",
    "    ax1.set_title('Sentiment Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 2. Emotion Distribution\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    emotion_counts = df['emotion'].value_counts().head(6)\n",
    "    ax2.barh(emotion_counts.index, emotion_counts.values, color='skyblue')\n",
    "    ax2.set_xlabel('Count')\n",
    "    ax2.set_title('Top Emotions Detected', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 3. Sentiment Score Distribution\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    ax3.hist(df['sentiment_polarity'], bins=30, color='purple', alpha=0.7, edgecolor='black')\n",
    "    ax3.axvline(df['sentiment_polarity'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {df[\"sentiment_polarity\"].mean():.3f}')\n",
    "    ax3.set_xlabel('Sentiment Polarity')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.set_title('Sentiment Score Distribution', fontsize=14, fontweight='bold')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # 4. Sentiment vs Engagement\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    sentiment_likes = df.groupby('sentiment')['like_count'].mean()\n",
    "    ax4.bar(sentiment_likes.index, sentiment_likes.values, \n",
    "            color=[colors[s] for s in sentiment_likes.index])\n",
    "    ax4.set_ylabel('Average Likes')\n",
    "    ax4.set_title('Average Likes by Sentiment', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 5. Sentiment Over Time (by hour) - if available\n",
    "    if 'hour_of_day' in df.columns:\n",
    "        ax5 = plt.subplot(2, 3, 5)\n",
    "        hourly_sentiment = df.groupby('hour_of_day')['sentiment_polarity'].mean()\n",
    "        ax5.plot(hourly_sentiment.index, hourly_sentiment.values, marker='o', color='orange', linewidth=2)\n",
    "        ax5.set_xlabel('Hour of Day')\n",
    "        ax5.set_ylabel('Average Sentiment')\n",
    "        ax5.set_title('Sentiment by Hour of Day', fontsize=14, fontweight='bold')\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax5 = plt.subplot(2, 3, 5)\n",
    "        ax5.text(0.5, 0.5, 'Hour data not available', ha='center', va='center')\n",
    "        ax5.set_title('Sentiment by Hour of Day', fontsize=14, fontweight='bold')\n",
    "        ax5.axis('off')\n",
    "    \n",
    "    # 6. Toxicity Distribution\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    toxicity_bins = pd.cut(df['toxicity_score'], bins=[0, 0.25, 0.5, 0.75, 1.0], \n",
    "                           labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "    toxicity_counts = toxicity_bins.value_counts()\n",
    "    ax6.bar(toxicity_counts.index, toxicity_counts.values, color='coral')\n",
    "    ax6.set_ylabel('Count')\n",
    "    ax6.set_title('Toxicity Level Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    viz_filename = f'{output_prefix}_visualizations.png'\n",
    "    plt.savefig(viz_filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"‚úì Saved visualizations to {viz_filename}\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Create word cloud for positive and negative comments\n",
    "    try:\n",
    "        from wordcloud import WordCloud\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "        \n",
    "        # Positive comments word cloud\n",
    "        positive_text = ' '.join(df[df['sentiment'] == 'Positive']['comment_text'].astype(str))\n",
    "        if positive_text.strip():\n",
    "            wordcloud_pos = WordCloud(width=800, height=400, background_color='white',\n",
    "                                     colormap='Greens', max_words=100).generate(positive_text)\n",
    "            axes[0].imshow(wordcloud_pos, interpolation='bilinear')\n",
    "            axes[0].set_title('Positive Comments Word Cloud', fontsize=16, fontweight='bold')\n",
    "            axes[0].axis('off')\n",
    "        \n",
    "        # Negative comments word cloud\n",
    "        negative_text = ' '.join(df[df['sentiment'] == 'Negative']['comment_text'].astype(str))\n",
    "        if negative_text.strip():\n",
    "            wordcloud_neg = WordCloud(width=800, height=400, background_color='white',\n",
    "                                     colormap='Reds', max_words=100).generate(negative_text)\n",
    "            axes[1].imshow(wordcloud_neg, interpolation='bilinear')\n",
    "            axes[1].set_title('Negative Comments Word Cloud', fontsize=16, fontweight='bold')\n",
    "            axes[1].axis('off')\n",
    "        \n",
    "        wordcloud_filename = f'{output_prefix}_wordclouds.png'\n",
    "        plt.savefig(wordcloud_filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"‚úì Saved word clouds to {wordcloud_filename}\")\n",
    "        plt.close()\n",
    "    except ImportError:\n",
    "        print(\"‚ö† WordCloud not available - skipping word clouds\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Could not create word clouds: {e}\")\n",
    "\n",
    "def print_analysis_report(df, insights):\n",
    "    \"\"\"Print comprehensive analysis report.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SENTIMENT ANALYSIS REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüìä OVERALL STATISTICS\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Total Comments Analyzed: {len(df):,}\")\n",
    "    print(f\"Average Sentiment Score: {insights['avg_sentiment_score']:.3f} \"\n",
    "          f\"({'Positive' if insights['avg_sentiment_score'] > 0 else 'Negative' if insights['avg_sentiment_score'] < 0 else 'Neutral'})\")\n",
    "    print(f\"Average Toxicity Score: {insights['avg_toxicity']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüí≠ SENTIMENT DISTRIBUTION\")\n",
    "    print(\"-\" * 80)\n",
    "    for sentiment, pct in sorted(insights['sentiment_distribution'].items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{sentiment:12s}: {pct:5.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüòä EMOTION DISTRIBUTION\")\n",
    "    print(\"-\" * 80)\n",
    "    for emotion, pct in sorted(insights['emotion_distribution'].items(), \n",
    "                               key=lambda x: x[1], reverse=True)[:6]:\n",
    "        print(f\"{emotion.capitalize():12s}: {pct:5.1f}%\")\n",
    "    \n",
    "    print(f\"\\nüëç ENGAGEMENT BY SENTIMENT\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Positive comments: {insights['avg_likes_positive']:.1f} avg likes\")\n",
    "    print(f\"Neutral comments:  {insights['avg_likes_neutral']:.1f} avg likes\")\n",
    "    print(f\"Negative comments: {insights['avg_likes_negative']:.1f} avg likes\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  TOXICITY ANALYSIS\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"High toxicity comments: {insights['high_toxicity_count']} ({insights['high_toxicity_pct']:.1f}%)\")\n",
    "    \n",
    "    # Show most positive and negative comments\n",
    "    print(f\"\\nüåü TOP 3 MOST POSITIVE COMMENTS\")\n",
    "    print(\"-\" * 80)\n",
    "    top_positive = df.nlargest(3, 'sentiment_polarity')[['author', 'comment_text', 'sentiment_polarity', 'like_count']]\n",
    "    for idx, row in top_positive.iterrows():\n",
    "        print(f\"\\nüë§ {row['author']} | Score: {row['sentiment_polarity']:.3f} | Likes: {row['like_count']}\")\n",
    "        comment_preview = row['comment_text'][:150] + \"...\" if len(row['comment_text']) > 150 else row['comment_text']\n",
    "        print(f\"   {comment_preview}\")\n",
    "    \n",
    "    print(f\"\\nüò† TOP 3 MOST NEGATIVE COMMENTS\")\n",
    "    print(\"-\" * 80)\n",
    "    top_negative = df.nsmallest(3, 'sentiment_polarity')[['author', 'comment_text', 'sentiment_polarity', 'like_count']]\n",
    "    for idx, row in top_negative.iterrows():\n",
    "        print(f\"\\nüë§ {row['author']} | Score: {row['sentiment_polarity']:.3f} | Likes: {row['like_count']}\")\n",
    "        comment_preview = row['comment_text'][:150] + \"...\" if len(row['comment_text']) > 150 else row['comment_text']\n",
    "        print(f\"   {comment_preview}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "def save_analyzed_data(df, insights, output_prefix='sentiment_analysis'):\n",
    "    \"\"\"Save analyzed data to files.\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save analyzed comments\n",
    "    output_csv = f'{output_prefix}_results_{timestamp}.csv'\n",
    "    df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\n‚úì Saved analyzed data to {output_csv}\")\n",
    "    \n",
    "    # Save insights as JSON\n",
    "    import json\n",
    "    insights_file = f'{output_prefix}_insights_{timestamp}.json'\n",
    "    with open(insights_file, 'w', encoding='utf-8') as f:\n",
    "        # Convert any numpy types to Python types for JSON serialization\n",
    "        insights_serializable = {}\n",
    "        for key, value in insights.items():\n",
    "            if isinstance(value, dict):\n",
    "                insights_serializable[key] = {k: float(v) if isinstance(v, (np.integer, np.floating)) else v \n",
    "                                             for k, v in value.items()}\n",
    "            elif isinstance(value, (np.integer, np.floating)):\n",
    "                insights_serializable[key] = float(value)\n",
    "            else:\n",
    "                insights_serializable[key] = value\n",
    "        json.dump(insights_serializable, f, indent=2)\n",
    "    print(f\"‚úì Saved insights to {insights_file}\")\n",
    "    \n",
    "    # Try to save to Excel if openpyxl is available\n",
    "    try:\n",
    "        import openpyxl\n",
    "        output_excel = f'{output_prefix}_results_{timestamp}.xlsx'\n",
    "        \n",
    "        with pd.ExcelWriter(output_excel, engine='openpyxl') as writer:\n",
    "            # Analyzed comments\n",
    "            df.to_excel(writer, sheet_name='Analyzed Comments', index=False)\n",
    "            \n",
    "            # Summary statistics\n",
    "            summary_data = {\n",
    "                'Metric': [\n",
    "                    'Total Comments',\n",
    "                    'Average Sentiment Score',\n",
    "                    'Positive Comments %',\n",
    "                    'Neutral Comments %',\n",
    "                    'Negative Comments %',\n",
    "                    'Average Toxicity',\n",
    "                    'High Toxicity Count',\n",
    "                    'Most Common Emotion'\n",
    "                ],\n",
    "                'Value': [\n",
    "                    len(df),\n",
    "                    f\"{insights['avg_sentiment_score']:.3f}\",\n",
    "                    f\"{insights['sentiment_distribution'].get('Positive', 0):.1f}%\",\n",
    "                    f\"{insights['sentiment_distribution'].get('Neutral', 0):.1f}%\",\n",
    "                    f\"{insights['sentiment_distribution'].get('Negative', 0):.1f}%\",\n",
    "                    f\"{insights['avg_toxicity']:.3f}\",\n",
    "                    insights['high_toxicity_count'],\n",
    "                    max(insights['emotion_distribution'], key=insights['emotion_distribution'].get)\n",
    "                ]\n",
    "            }\n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "            summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "            \n",
    "            # Sentiment breakdown\n",
    "            sentiment_breakdown = df.groupby('sentiment').agg({\n",
    "                'comment_id': 'count',\n",
    "                'like_count': 'mean',\n",
    "                'sentiment_polarity': 'mean',\n",
    "                'toxicity_score': 'mean'\n",
    "            }).round(2)\n",
    "            sentiment_breakdown.columns = ['Count', 'Avg Likes', 'Avg Sentiment', 'Avg Toxicity']\n",
    "            sentiment_breakdown.to_excel(writer, sheet_name='Sentiment Breakdown')\n",
    "            \n",
    "        print(f\"‚úì Saved Excel file to {output_excel}\")\n",
    "    except ImportError:\n",
    "        print(\"‚ö† openpyxl not available - Excel export skipped\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Could not create Excel file: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"YouTube Comments Sentiment Analysis\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading comments data...\")\n",
    "    import glob\n",
    "    csv_files = glob.glob(INPUT_FILE)\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"‚ùå No CSV files found matching pattern: {INPUT_FILE}\")\n",
    "        print(\"\\nPlease run the comments extraction script first.\")\n",
    "        return\n",
    "    \n",
    "    # Use the most recent file\n",
    "    latest_file = max(csv_files, key=lambda x: x)\n",
    "    print(f\"‚úì Loading: {latest_file}\\n\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(latest_file)\n",
    "        print(f\"‚úì Loaded {len(df):,} comments\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading file: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Perform sentiment analysis\n",
    "    df = analyze_comments(df)\n",
    "    \n",
    "    # Generate insights\n",
    "    print(\"\\nGenerating insights...\")\n",
    "    insights = generate_insights(df)\n",
    "    \n",
    "    # Create visualizations\n",
    "    create_visualizations(df)\n",
    "    \n",
    "    # Print report\n",
    "    print_analysis_report(df, insights)\n",
    "    \n",
    "    # Save results\n",
    "    save_analyzed_data(df, insights)\n",
    "    \n",
    "    print(\"\\n‚úÖ Sentiment analysis complete!\")\n",
    "    print(\"\\nFiles generated:\")\n",
    "    print(\"  üìÑ CSV with sentiment scores\")\n",
    "    print(\"  üìä Excel with detailed breakdown\")\n",
    "    print(\"  üìà Visualization charts\")\n",
    "    print(\"  üí≠ Word clouds (if available)\")\n",
    "    print(\"  üìã Insights JSON\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
